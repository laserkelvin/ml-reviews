{"componentChunkName":"component---node-modules-gatsby-theme-garden-src-templates-local-file-js","path":"/notes/autoencoders","result":{"data":{"file":{"childMdx":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"autoencoders\"), mdx(\"p\", null, \"A type of machine learning [\", \"[architecture]\", \"] that encodes data into a self-learned representation.\"));\n}\n;\nMDXContent.isMDXComponent = true;","outboundReferences":[],"inboundReferences":[{"__typename":"Mdx","body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": \"needs-expanding\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"variational autoencoder\"), mdx(\"p\", null, \"A probabilistic [\", \"[architecture]\", \"] from the family of [\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"autoencoders\",\n    \"title\": \"autoencoders\"\n  }, \"autoencoders\"), \"]. The self-learned representation is used to parameterize a probability distribution (i.e. the [\", \"[posterior distribution]\", \"]), from which a decoder can draw samples from to generate a range of outputs. We can either directly predict the mean of the distribution, or perform sampling over the distribution to obtain #uncertainty estimates.\"), mdx(\"p\", null, \"Is a sub-category of [\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"bayesian-neural-networks\",\n    \"title\": \"bayesian-neural-networks\"\n  }, \"bayesian-neural-networks\"), \"], whereby variational inference is done, usually in contrast to full [\", \"[MCMC]\", \"] sampling.\"), mdx(\"p\", null, \"Problem with VAEs is that they are susceptible to [\", \"[posterior collapse]\", \"], where a single output is produced regardless: this is when the decoder ignores the latent variable completely, and becomes a deterministic model.\"), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html\"\n  }, \"This blog by Lilian Weng\"), \" has an incredibly in-depth description of the theory behind VAEs.\"), mdx(\"h2\", null, \"Types of VAEs\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"[\", \"[beta-VAE]\", \"] provides a tuning parameter, \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"\\u03B2\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\beta\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.8888799999999999em\",\n      \"verticalAlign\": \"-0.19444em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.05278em\"\n    }\n  }, \"\\u03B2\"))))), \", that attempts to force disentangling of the latent vector into unit Gaussian priors. Relatively easy to understand and implement, however quite readily suffers from [\", \"[posterior collapse]\", \"].\")), mdx(\"p\", null, \"[posterior collapse]\", \": posterior collapse \\\"posterior collapse\\\"\\n\", \"[posterior collapse]\", \": posterior collapse \\\"posterior collapse\\\"\"), mdx(\"p\", null, \"[posterior collapse]\", \": posterior collapse \\\"posterior collapse\\\"\\n\", \"[posterior collapse]\", \": posterior collapse \\\"posterior collapse\\\"\"), mdx(\"p\", null, \"[posterior collapse]\", \": posterior collapse \\\"posterior collapse\\\"\\n\", \"[posterior collapse]\", \": posterior collapse \\\"posterior collapse\\\"\"));\n}\n;\nMDXContent.isMDXComponent = true;","parent":{"__typename":"File","id":"0dff8916-86db-5c4c-bdee-c0c7be1cb322","fields":{"slug":"/notes/variational autoencoder","title":"variational autoencoder"}}}]},"fields":{"slug":"/notes/autoencoders","title":"autoencoders"}}},"pageContext":{"id":"a4f94698-6ed6-5e36-bae3-d4b2fc873702"}},"staticQueryHashes":["2098632890","2221750479","2468095761"]}