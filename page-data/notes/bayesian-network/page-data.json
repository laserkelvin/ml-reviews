{"componentChunkName":"component---node-modules-gatsby-theme-garden-src-templates-local-file-js","path":"/notes/bayesian-network","result":{"data":{"file":{"childMdx":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"bayesian-network\"\n  }, \"bayesian-network\"), mdx(\"h2\", {\n    \"id\": \"definition\"\n  }, \"Definition\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Not to be confused with [\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"bayesian-neural-networks\",\n    \"title\": \"bayesian-neural-networks\"\n  }, \"bayesian-neural-networks\"), \"], this type of model is basically analogous to a [\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"decision-tree\",\n    \"title\": \"decision-tree\"\n  }, \"decision-tree\"), \"], albeit as a directed acyclic \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/graph\",\n    \"title\": \"graph\"\n  }, \"[[graph]]\"), \".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Each node corresponds to a variable, with the directed edges their probabilities (transition probabilities like in \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/markov-decision-process\",\n    \"title\": \"markov-decision-process\"\n  }, \"[[markov-decision-process]]\"), \"?)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"As an expression, with variable $X_i$, each edge corresponds to the [\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"conditional-distribution\",\n    \"title\": \"conditional-distribution\"\n  }, \"conditional-distribution\"), \"] likelihood $P(X_i \\\\vert Pa(X_i))$, where $Pa(X_i)$ are the parent nodes of $X_i$, i.e. every decision made prior to arriving at $X_i$.\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"$Pa(X_i)$ is actually computed with the chain rule; we have to evaluate the conditional likelihood of every event prior to $X_i$.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"From the \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/decision-making-book\",\n    \"title\": \"decision-making-book\"\n  }, \"[[decision-making-book]]\"), \", we can describe how events that happen one after another as these Bayesian networks:\")), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"366px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/ml-reviews/static/816ea670845de70be326d595d3ecd196/8b153/2021-08-25-08-15-57.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"216.42857142857142%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAArCAIAAAD3xz8iAAAACXBIWXMAAA7DAAAOwwHHb6hkAAAEzUlEQVRIx6WW6XaiWhCFef/H6uSmtdsRkBlBZZ5EcU5i7ocYo4mu++NWXBE8p6r22TUdYT6fb9abNbJZLxaL7a6W9/d3/s+L+XJZ7Xe77Xa72Wy22x2blstlvVSWu/1e0DRVkuThYND63e52u4HvG4ZZlgvPm/V7vXa7rahav99nyXVdWZKGomjo+svLSxhFQhxFiqI6Y9swDEVRRiPFNE3P9+M4tixLEsWnX78m0+nEdXnVVFXTNP0keV4I8yKX5RELkwl7Zq7r9HpdnvIsc8ZjvJmGEYRIMGWZz2wahpGua6ZlCbZtg7zb7Q0leTweR3H0++UfUZQN3UiSeOoFg35/NpuNRiPTsk3TUpURvj2w+b6gyLJh2YDHimXbURRyJN8PPc+bngTnGLX52Lbv10pZlrE3STLBsS1FM3gfyTImJ64DK0EQyZIo1ccxOZ4kiYOBiLnZbDLzfKjWVSVJcwFj8AlROjjjeOw4UCJJUlEU9a+K+qfdWm8PYeBhdDx2YGcwGMiyDAYhy1JVVXVNx3kURZyfEwaBD1r4N6DFstabTZIk4IZrrLNk2+MkSQUO2e91NMPK0gR7umHygMdO508YZ+CKwiBNY9dxiHarTS50CHWn04V4Aai4wryOHx0MBjlknnzyZ1vWyZM7LxcfHx/HjxsRasy1mq6oKl9kG1gs05BkGciGfpYwjNn9fpJG83g8Ch//QwQM8FXOCx+WfJ+kb6zi4VjLe/PMj+v1ijwjpd/e3m6UiyIHJGlEATXK1x6a1+ViwXEURXt7fT0rN19VVaUpwcoovUcgV6tVmqbEjM1fytvdlsRfLhfLqiKNvrltZL/fB0FAaVP4YRC8npwLjVvyiRPnRUElXo50jZk9tIoiz/AAQFCclVmGKvSBRNTueoYzqj0/7SERb0LFGrnBgTEPvIvD5j+rJNLqJEC4WBd+IizLstHnYE3A0LyQdB2L70nSLOR5TpdKyOk4fn5+Xq3WP+P3UBn8dKUN/XRZkdhEiHCUi8Wy5izntSH1vjLIq2qZZ2m5qOZFgSG6gu/TywJKyDTM/WF/R7kRDJNteCIEVM5dzDfKx09puCUZmAeQTxQI5IX8+uERYZeoXP944f9+VeEBSuI4eXp6wtU1kGYDWQXt8A+Ki+naM2VI0pRzumIJN3ePdzgcaMNUZVVRmOFXbuOWwPreDH0IvZvbwIF2xst6u8vS9Ca3SYYmN4nho5IEHXkGi82ec24TejoE84teQkM9fNb6Tdqs1/R8dzKBOXrm4QIbS4wSjlT3E9MkSNeBvTDKDIEVyp4pUq23Z7aPn5uaXlUXQ/25Q9wV/R8PM+yufMNSKzO2B0NR19R+f8Ao8j1PrGXI6CJy/9F6GcJRnDBNTiMqYKYTRnLfHjt5np2hntvwj5KMo5Cp0+38tZjOtt0fDJgVPIjDoTwatVqtMIofFkY9/qdThhMzidbNKSaTGROHWwrOCQ/91HXcLK+lOfKXMp3YcR2aG60P56ahuZOpzphXFVEacQmBiF6vN1IUbH2DIOBWVZVmxNUDQTXCwKfiwYzzoShRz1wU7rehegikKZcgdnPVQbiBhGF9G4N56oTzX/LxB2HMZ8/nWjOsAzRkvne6XSLX+duBOcedMM0B8u20jfwL3GtjNqMIcBsAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"2021 08 25 08 15 57\",\n    \"title\": \"2021 08 25 08 15 57\",\n    \"src\": \"/ml-reviews/static/816ea670845de70be326d595d3ecd196/8b153/2021-08-25-08-15-57.png\",\n    \"srcSet\": [\"/ml-reviews/static/816ea670845de70be326d595d3ecd196/0d3e1/2021-08-25-08-15-57.png 140w\", \"/ml-reviews/static/816ea670845de70be326d595d3ecd196/6b1e2/2021-08-25-08-15-57.png 281w\", \"/ml-reviews/static/816ea670845de70be326d595d3ecd196/8b153/2021-08-25-08-15-57.png 366w\"],\n    \"sizes\": \"(max-width: 366px) 100vw, 366px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n  \"), \"\\n    \")), mdx(\"h2\", {\n    \"id\": \"inference\"\n  }, \"Inference\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The utility of Bayesian networks is in Bayesian inference: inferring the likelihood of one event given some evidence. With the satellite example above, an example would be observing trajectory deviation, and asking what the likelihood of it being related to battery or solar failures.\")), mdx(\"p\", null, \"#idea to parameterize Bayesian networks with graph neural networks; good application for #intel-research? This would relate to problems where the probabilities are not known, or for networks that are much larger than what could be done with sampling methods.\"), mdx(\"p\", null, \"#idea to use Bayesian networks to infer molecule detections? Phrase detection problem as a Bayesian network, where the probability of detection for each molecule is a conditional probability.\"));\n}\n;\nMDXContent.isMDXComponent = true;","outboundReferences":[{"__typename":"Mdx","body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"bayesian-neural-networks\"\n  }, \"bayesian-neural-networks\"), mdx(\"p\", null, \"\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/neural-networks\",\n    \"title\": \"neural networks\"\n  }, \"[[neural networks]]\"), \" in a \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/bayesian\",\n    \"title\": \"bayesian\"\n  }, \"[[bayesian]]\"), \" formalism. The general idea is that the initialization schemes for neural networks act as priors, and by training the network, we end up with a parameterized \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/posterior-distribution\",\n    \"title\": \"posterior distribution\"\n  }, \"[[posterior distribution]]\"), \".\"));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"aliases":[]},"parent":{"__typename":"File","id":"f9da958d-5c93-5082-898e-0f3d0a4b0dd1","fields":{"slug":"/notes/bayesian-neural-networks","title":"bayesian-neural-networks"}}},{"__typename":"Mdx","body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"decision-tree\"\n  }, \"decision-tree\"), mdx(\"h2\", {\n    \"id\": \"storing-joint-probabilities\"\n  }, \"Storing joint probabilities\"), mdx(\"p\", null, \"From \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/decision-making-book\",\n    \"title\": \"decision-making-book\"\n  }, \"[[decision-making-book]]\"), \", one way of storing joint discrete probabilities is with a decision tree. For example, the joint probability of $P(x,y,z)$ factored as the product of three independent variables that can be zero or one, $P(x) \\\\times P(y) \\\\times P(z)$. Instead of storing the full table of products, we represent things as a #tree/#graph, where each node is a variable, and the edges are whether that variable is zero or one.\"), mdx(\"p\", null, \"The result is storing only five probabilities, as opposed to eight. The savings scale well in the limit of many possibilities/variables.\"), mdx(\"p\", null, mdx(\"img\", {\n    parentName: \"p\",\n    \"src\": \"2021-08-25-07-40-09.png\",\n    \"alt\": null\n  })));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"aliases":[]},"parent":{"__typename":"File","id":"e8b4b47a-9edd-5123-9f99-5ca526838a73","fields":{"slug":"/notes/decision-tree","title":"decision-tree"}}},{"__typename":"Mdx","body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": \"probability, distributions\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"conditional-distribution\"\n  }, \"conditional-distribution\"), mdx(\"p\", null, \"A \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/probability-distribution\",\n    \"title\": \"probability-distribution\"\n  }, \"[[probability-distribution]]\"), \" related to the [\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"marginal-distribution\",\n    \"title\": \"marginal-distribution\"\n  }, \"marginal-distribution\"), \"], as the possible values of one variable given others.\"), mdx(\"p\", null, \"$$P(x \\\\vert y) = \\\\frac{P(x,y)}{P(y)}$$\"), mdx(\"p\", null, \"The law of total probability is given as $P(x) = \\\\sum_y P(x \\\\vert y)P(y)$, as a re-arranged version of the conditional likelihood. This is related to [\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"bayes-rule\",\n    \"title\": \"bayes-rule\"\n  }, \"bayes-rule\"), \"].\"), mdx(\"p\", null, \"Compared to marginal distributions, conditional distributions can be much more efficiently factored as \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/decision-trees\",\n    \"title\": \"decision trees\"\n  }, \"[[decision trees]]\"), \".\"), mdx(\"p\", null, \"A [\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"linear-gaussian-model\",\n    \"title\": \"linear-gaussian-model\"\n  }, \"linear-gaussian-model\"), \"] is an example of a conditional distribution model.\"));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"aliases":[]},"parent":{"__typename":"File","id":"4b6e0da8-33e0-5618-acf9-96d0b38159f6","fields":{"slug":"/notes/conditional-distribution","title":"conditional-distribution"}}}],"inboundReferences":[]},"fields":{"slug":"/notes/bayesian-network","title":"bayesian-network"}}},"pageContext":{"id":"a47525cc-f6dc-5ef7-9215-50f03cdf6bf6"}},"staticQueryHashes":["2098632890","2221750479","2468095761"]}