{"componentChunkName":"component---node-modules-gatsby-theme-garden-src-templates-local-file-js","path":"/notes/scoring rule","result":{"data":{"file":{"childMdx":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"scoring-rule\"\n  }, \"scoring rule\"), mdx(\"p\", null, \"See:\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"T. Gneiting and A. E. Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477):359\\u2013378, 2007\\nfor a review.\")), mdx(\"p\", null, \"Using the formalism in [\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"scalable-uncertainties-from-deep-ensembles\",\n    \"title\": \"scalable-uncertainties-from-deep-ensembles\"\n  }, \"scalable-uncertainties-from-deep-ensembles\"), \"]:\"), mdx(\"p\", null, \"Ascribe a numerical score to a predictive distribution $p_\\\\theta (y \\\\vert x)$ that rewards calibrated predictions; higher is better. \"), mdx(\"p\", null, \"Scoring function $S(p\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"\\\\theta, (y, x))$ that evaluates the quality of predictive distribution $p\"), \"\\\\theta(y \\\\vert x)$ relative to an event $y \\\\vert x \\\\sim q(y \\\\vert x)$ with $q$ being the true distribution. i.e. assess how well distribution $p\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"\\\\theta$ encodes the true process $q$, given data $y \\\\vert x$ is drawn from $q$. $S$ is maximized when $p\"), \"\\\\theta \\\\rightarrow q$.\"));\n}\n;\nMDXContent.isMDXComponent = true;","outboundReferences":[{"__typename":"Mdx","body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": \"papers, uncertainty, deep-ensembles, pytorch, julia\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"scalable-uncertainties-from-deep-ensembles\"\n  }, \"scalable-uncertainties-from-deep-ensembles\"), mdx(\"h2\", {\n    \"id\": \"arxiv\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"https://arxiv.org/abs/1612.01474v3\"\n  }, \"arxiv\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"First written\"), \": Aug/20/2021, 09:32:40\"), mdx(\"h2\", {\n    \"id\": \"summary\"\n  }, \"Summary\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"While \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/neural-networks\",\n    \"title\": \"neural networks\"\n  }, \"[[neural networks]]\"), \" are good at a wide range of tasks, they aren't good at knowing \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"when\"), \" and \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"what\"), \" they don't know.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Part of the problem with this is that there is no ground truth for uncertainty; not something that can be easily testable. \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Same problem associated with quantifying #generalization since the data we need is by definition not available to us.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"This paper looks at using two ways to measure uncertainty from neural networks: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/calibration\",\n    \"title\": \"calibration\"\n  }, \"[[calibration]]\"), \" and \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/domain-shift\",\n    \"title\": \"domain shift\"\n  }, \"[[domain shift]]\"), \" or \\\"out-of-distribution examples\\\".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Interpretation of \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/dropout\",\n    \"title\": \"dropout\"\n  }, \"[[dropout]]\"), \" uncertainties by Gal and Ghahramani as both \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/MCMC\",\n    \"title\": \"MCMC\"\n  }, \"[[MCMC]]\"), \" sampling, as well as \\\"creation\\\" of [\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"ensembles\",\n    \"title\": \"ensembles\"\n  }, \"ensembles\"), \"] of neural networks.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Fundamentally, the question is whether or not ensembles of neural networks are any good at providing good uncertainty estimates.\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"blockquote\", {\n    parentName: \"li\"\n  }, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/bayesian-model-averaging\",\n    \"title\": \"bayesian model averaging\"\n  }, \"[[bayesian model averaging]]\"), \" assumes that the true model lies within the hypothesis class of the prior, and performs \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"soft model selection\"), \" to find the single best model within the hypothesis class.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"blockquote\", {\n    parentName: \"li\"\n  }, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"...ensembles can be expected to be better when the true model does not lie within the hypothesis class.\"))))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A deep ensemble is supposed to be easier to implement and train than [\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"bayesian-neural-networks\",\n    \"title\": \"bayesian-neural-networks\"\n  }, \"bayesian-neural-networks\"), \"], either variational or MCMC.\")), mdx(\"h3\", {\n    \"id\": \"calibration\"\n  }, \"Calibration\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"This figure shows the performance of the approach w.r.t. a simple 1D regression task, where $y = x^3 + \\\\varepsilon$ \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"561px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/ml-reviews/static/d3557e106e6bf95b5ddd8b8da67259ff/f0685/2021-08-20-10-38-27.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"38.57142857142858%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABf0lEQVQoz01Si26DMAzk/z+uK4XSFzBV5RVCgEIbIEBvdtikIVm2L3fGB3GmacIwDDCzgTEGWo+2Hwmf5xkTYa+X3jhUMzYMpNEDWMsYa7n+fD5wtNYYf8ld26MULQ3oMY4jzDjhcc+R55IENID6vuuRJhJClNSPmEmnZI0ovIMfp+97SKHQqhbv7m0HCyGR3DM0RHzSS7R+I3nkEGmBpmqx0JaPJMU3DXk9e1SlQl03NI425FWb5oVlma0dYybU6mltrp/V2mEXSnV0Nlseh5QVDaHh62KDtXbDsiyJXCFJErJbElHayPMMks8qRXVOYgVRCBRFAdbUdW3rgs7+cP5Mjrvf43gM4Hs+Dq6LIAhs9jwP3uEA3/fxtdvB3bvEO264t+F71lJm3m73hbZ9wmFSHMcIwxDn0wnn8xmXy4XyyYqvVHMfhjfEUYzb7YYoimxciHu9Xi3Gwf/DYatsKeWc5dZ6VVXIstTi3Kdpiq7rtititivyP7Zvb7CuK34AxPVYhKbTrVgAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"2021 08 20 10 38 27\",\n    \"title\": \"2021 08 20 10 38 27\",\n    \"src\": \"/ml-reviews/static/d3557e106e6bf95b5ddd8b8da67259ff/410f3/2021-08-20-10-38-27.png\",\n    \"srcSet\": [\"/ml-reviews/static/d3557e106e6bf95b5ddd8b8da67259ff/0d3e1/2021-08-20-10-38-27.png 140w\", \"/ml-reviews/static/d3557e106e6bf95b5ddd8b8da67259ff/6b1e2/2021-08-20-10-38-27.png 281w\", \"/ml-reviews/static/d3557e106e6bf95b5ddd8b8da67259ff/410f3/2021-08-20-10-38-27.png 561w\", \"/ml-reviews/static/d3557e106e6bf95b5ddd8b8da67259ff/f0685/2021-08-20-10-38-27.png 835w\"],\n    \"sizes\": \"(max-width: 561px) 100vw, 561px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n  \"), \"\\n    \"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"First panel is empirical variance. \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Probably not much because each NN converges to the same weights?\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Second panel is training NLL using a single NN, third smooths the process with adversarial training.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Final panel combines NLL + AT with five NNs.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Other comparison made is on a few tabular datasets; deep ensembles typically have larger RMSE but better NLL.\")), mdx(\"h3\", {\n    \"id\": \"domain-shift\"\n  }, \"Domain shift\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Test predictions on data from \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"unseen\"), \" classes: uncertainties should be proportional to distance from training data.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Model is trained on a standard \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/MNIST\",\n    \"title\": \"MNIST\"\n  }, \"[[MNIST]]\"), \" train/test split, however an additional testing class of \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/NotMNIST\",\n    \"title\": \"NotMNIST\"\n  }, \"[[NotMNIST]]\"), \".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"blockquote\", {\n    parentName: \"li\"\n  }, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \" We do not have access to the true conditional probabilities, but we expect the predictions to be closer to uniform on unseen classes compared to the known classes where the predictive probabilities should concentrate on the true targets\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The left panel shows how the distribution in \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/entropy\",\n    \"title\": \"entropy\"\n  }, \"[[entropy]]\"), \" is much broader/uniform for the ensemble + AT case, compared to the dropout uncertainties which are also much more discrete. \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"355px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"80%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAACNElEQVQ4y42UW3ObMBCF/f//TB86fel7n9rpJG0mNW58wWDuIEAgcRE6XckmxYlnYs0wsOjo26PLagVqeZZit93CcRys1w62ux0YK3E4uJBSYm5SCmxI4/u+1TrOBru9a+OqrKxmpZTCMAyQbQshBDpeQVYMg2gtbBxHaK3tY76HTqLvOoiGoykyqxdNY7UTsSxQTxOmIrYZpppBFSkUQeu6xmT65of6dZVDyxYYehqTUlygrUr0ZMpoVsoIxwFj4p9dtA0GA6KMSZKgJdc20QWo8hhTy6HIbV9XUDQzXjJkeW7Hn4GUTaUBNA3oeY2uLC24oakIIf8DtQFGlKyF6nvSMQvtKCnnfOGw7zAS8Fabp6vfAG81ZdfwAlQX4LwB82PXmN4GqkychZg6+U5r+z8CzlmvgbQ0/TVw/r4CjncDyeE9wI8dKrtp/PkBXRrf51BfOrB4vwUW37+hz9OzxmzWLaCiKZjde91ZOkZ6sXPLg80ef6ANTxTrV/28JKaSVva3GjD4OygqJ+Hu0bxsIE6+UV4NMo2vfyP8+gXe509oSGtmt2yrqqqQUUVET49I188okxgNHdI68JEdjyiyDEJ26Okg51QN8fYF0cNPROs/iJ9+IfvrgMUxqqJAUbAz0Nw2e/cI1/PgnQJ7e7gUB0EATmXYUrV0dCEwVuDo+TiQzvVPiLOcYg97up04VZgFzlZzcpKSuyiibFR6YRCiZOxdNZgEWZogDEMwcmXqPSaH85r+A51Q2Fnq3NaHAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"2021 08 20 11 14 39\",\n    \"title\": \"2021 08 20 11 14 39\",\n    \"src\": \"/ml-reviews/static/ecad5890bae970f3f7bbfc2ef55b4fcd/526ee/2021-08-20-11-14-39.png\",\n    \"srcSet\": [\"/ml-reviews/static/ecad5890bae970f3f7bbfc2ef55b4fcd/0d3e1/2021-08-20-11-14-39.png 140w\", \"/ml-reviews/static/ecad5890bae970f3f7bbfc2ef55b4fcd/6b1e2/2021-08-20-11-14-39.png 281w\", \"/ml-reviews/static/ecad5890bae970f3f7bbfc2ef55b4fcd/526ee/2021-08-20-11-14-39.png 355w\"],\n    \"sizes\": \"(max-width: 355px) 100vw, 355px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Define a \\\"confidence\\\" value $0 \\\\leq \\\\tau \\\\leq 1$ defined as the highest probability of a class. \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"It shouldn't always be one for unseen examples.\"), \" \")), mdx(\"h2\", {\n    \"id\": \"strategy\"\n  }, \"Strategy\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"blockquote\", {\n    parentName: \"li\"\n  }, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Given the input features $x$, we use a neural network to model the probabilistic predictive distribution $p_\\\\theta(y \\\\vert x)$ over the labels, where $\\\\theta$ are the parameters of the NN.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Three steps:\", mdx(\"ol\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Use a proper \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/scoring-rule\",\n    \"title\": \"scoring rule\"\n  }, \"[[scoring rule]]\"), \"; $-S(p_\\\\theta, y \\\\vert x) \\\\in \\\\mathcal{L}$ where $S$ can be \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/maximum-likelihood\",\n    \"title\": \"maximum likelihood\"\n  }, \"[[maximum likelihood]]\"), \"/\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/mean-squared-error\",\n    \"title\": \"mean squared error\"\n  }, \"[[mean squared error]]\"), \"/\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/softmax\",\n    \"title\": \"softmax\"\n  }, \"[[softmax]]\"), \". MSE is also known as the \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/Brier-score\",\n    \"title\": \"Brier score\"\n  }, \"[[Brier score]]\"), \".\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/Adversarial-training\",\n    \"title\": \"Adversarial training\"\n  }, \"[[Adversarial training]]\"), \" to smooth the predictive distribution\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Train an ensemble \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"lol\"))))), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"561px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/ml-reviews/static/0e9de60ce54afb3da942ba2848707d3d/ace37/2021-08-20-10-33-20.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"30%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABGElEQVQY001Q2W6EMAzk//9t0VZbCIHlhhDCVegD2iIxtU1X6oPlI5PxjD3f93G7+QjDECpU0FpDRxEiCu7jWCMIAiRJgvT5FFzTNJimCeM4YhgGCecclmWB5/oeSikkcXyR/RHGlBV9fpNH6spMGBE+z3JUZYk8z2HaFlVVoaTe+3m90NKgpgFn21kCVsjSFF3Xoe8dhZXaWiuqRlJkuZf3XtTO8yzh8VDRdud6ZLSVAdu6ytaV8nEc2Pcd53nie9vE2kDBxMYYEcEzrp0b4H2R74/7HY/HA58UbLmuSrKnEdLtirwQdRuRsT1+Z6vxvxPxnB0KIR+SB0VRiiq21ZlODs3HN+ay9bbMeLZY17Xg5Vxcs0LC/QIRbbbSQ3ds4AAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"2021 08 20 10 33 20\",\n    \"title\": \"2021 08 20 10 33 20\",\n    \"src\": \"/ml-reviews/static/0e9de60ce54afb3da942ba2848707d3d/410f3/2021-08-20-10-33-20.png\",\n    \"srcSet\": [\"/ml-reviews/static/0e9de60ce54afb3da942ba2848707d3d/0d3e1/2021-08-20-10-33-20.png 140w\", \"/ml-reviews/static/0e9de60ce54afb3da942ba2848707d3d/6b1e2/2021-08-20-10-33-20.png 281w\", \"/ml-reviews/static/0e9de60ce54afb3da942ba2848707d3d/410f3/2021-08-20-10-33-20.png 561w\", \"/ml-reviews/static/0e9de60ce54afb3da942ba2848707d3d/ace37/2021-08-20-10-33-20.png 666w\"],\n    \"sizes\": \"(max-width: 561px) 100vw, 561px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n  \"), \"\\n    \")), mdx(\"h3\", {\n    \"id\": \"regression\"\n  }, \"Regression\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Typical regression NN will minimize MSE, i.e. the output corresponds to the regression mean. Here, the authors suggest to output \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"two values\"), \": the predicted mean and variance. The variance is done via $\\\\sigma = (1 + \\\\exp(x)) + \\\\varepsilon$ where $\\\\varepsilon$ is a small positive number of numerical stability.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/negative-log-likelihood\",\n    \"title\": \"negative log-likelihood\"\n  }, \"[[negative log-likelihood]]\"), \" criterion used:\\n$$ -\\\\log p\", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"\\\\theta(y_n \\\\vert x_n) = \\\\frac{\\\\log \\\\sigma^2\"), \"\\\\theta(x)}{2} + \\\\frac{(y - \\\\mu\", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"\\\\theta(x))^2}{2\\\\sigma^2\"), \"\\\\theta(x)} + C$$\")), mdx(\"p\", null, \"Julia implementation:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-julia\"\n  }, \"begin\\n  function nll(mu, var, y)\\n    return log(var^2) / 2 + (y - mu)^2 / (2 * var^2) + eps(Float32)\\n  end\\nend\\n\")), mdx(\"h3\", {\n    \"id\": \"adversarial-training\"\n  }, \"Adversarial training\"), mdx(\"p\", null, \"Basically use conventional \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/fast-gradient-sign-method\",\n    \"title\": \"fast gradient sign method\"\n  }, \"[[fast gradient sign method]]\"), \", or \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/virtual-adversarial-training\",\n    \"title\": \"virtual adversarial training\"\n  }, \"[[virtual adversarial training]]\"), \".\"), mdx(\"h3\", {\n    \"id\": \"ensemble-training\"\n  }, \"Ensemble training\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Choice between \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/decision-trees\",\n    \"title\": \"decision trees\"\n  }, \"[[decision trees]]\"), \" or [\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"boosting\",\n    \"title\": \"boosting\"\n  }, \"boosting\"), \"]; the authors chose the former because they are much better for distributed training.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"There is an extensive discussion into decorrelation of trees with \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/bootstrapping\",\n    \"title\": \"bootstrapping\"\n  }, \"[[bootstrapping]]\"), \"; the authors instead just use random initialization of the parameters, and shuffle the training data points for each \\\"tree\\\" in practice.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Apparently [\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"stochastic-multiple-choice-learning\",\n    \"title\": \"stochastic-multiple-choice-learning\"\n  }, \"stochastic-multiple-choice-learning\"), \"] helps with de-correlating the NN \\\"trees\\\".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Ensemble as a uniformly-weighted \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/mixture-model\",\n    \"title\": \"mixture model\"\n  }, \"[[mixture model]]\"), \"\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"For classification, averaging the predicted probabilities\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"For regression, Gaussian mixture, with a mixture mean $\\\\mu\", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"*(x)$ and variance $\\\\sigma\"), \"*(x)$\")))), mdx(\"p\", null, \"$$\\\\mu\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"*(x) = \\\\frac{1}{M}\\\\sum_m \\\\mu\"), \"{\\\\theta_m}(x)$$\"), mdx(\"p\", null, \"$$\\\\sigma\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"*^2(x) = \\\\frac{1}{M}\\\\sum_m(\\\\sigma^2\"), \"{\\\\theta\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"m}(x) + \\\\mu^2\"), \"{\\\\theta\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"m}(x)) - \\\\mu\"), \"*^2(x)$$\"), mdx(\"p\", null, \"A naive Julia implementation:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-julia\"\n  }, \"begin\\n  \\\"\\\"\\\"Reduce the predictions from each tree into an ensemble\\n  mean and variance. Assumes `means` and `vars` are a list\\n  of vectors.\\n  \\\"\\\"\\\"\\n  function reduce_ensemble(means, vars)\\n    means = reduce(hcat, means)\\n    ensemble_mean = mean(means, dims=2)\\n    vars = reduce(hcat, vars)\\n    ensemble_var = mean(means.^2 .+ vars.^2, dims=2) .- ensemble_mean.^2\\n    return ensemble_mean, ensemble_var\\n  end\\n\\n  \\\"\\\"\\\"Given a struct `ensemble`, and inputs `x`, we loop over\\n  each tree in the ensemble/forest (also assuming tree is a functor)\\n  and collect up all of the means/variances predicted by each tree.\\n  We then bag the results.\\n  \\\"\\\"\\\"\\n  function predict(x, ensemble)\\n    means, vars = [], []\\n    for tree in ensemble.forest\\n      mean, var = tree(x)\\n      push!(means, mean)\\n      push!(vars, var)\\n    end\\n    ensemble_mean, ensemble_var = reduce_ensemble(means, vars)\\n  end\\nend\\n\")), mdx(\"h2\", {\n    \"id\": \"comments\"\n  }, \"Comments\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Good for #distributed training; maybe a good use case for #intel-research.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"5 NN's seem good enough for uncertainty estimation, but not sure how this really actually scales for real tasks.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"How does it really fit into a hypothesis testing context?\")), mdx(\"p\", null, \"[scoring rule]\", \": scoring rule \\\"scoring rule\\\"\\n\", \"[maximum likelihood]\", \": maximum likelihood \\\"maximum likelihood\\\"\\n\", \"[Adversarial training]\", \": Adversarial training \\\"Adversarial training\\\"\\n\", \"[negative log-likelihood]\", \": negative log-likelihood \\\"negative log-likelihood\\\"\\n\", \"[fast gradient sign method]\", \": fast gradient sign method \\\"fast gradient sign method\\\"\\n\", \"[virtual adversarial training]\", \": virtual adversarial training \\\"virtual adversarial training\\\"\"));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"aliases":[]},"parent":{"__typename":"File","id":"3f62869b-c6df-5c74-bc98-e7fcbbfb615d","fields":{"slug":"/notes/scalable-uncertainties-from-deep-ensembles","title":"scalable-uncertainties-from-deep-ensembles"}}}],"inboundReferences":[{"__typename":"Mdx","body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": \"papers, uncertainty, deep-ensembles, pytorch, julia\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"scalable-uncertainties-from-deep-ensembles\"\n  }, \"scalable-uncertainties-from-deep-ensembles\"), mdx(\"h2\", {\n    \"id\": \"arxiv\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"https://arxiv.org/abs/1612.01474v3\"\n  }, \"arxiv\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"First written\"), \": Aug/20/2021, 09:32:40\"), mdx(\"h2\", {\n    \"id\": \"summary\"\n  }, \"Summary\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"While \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/neural-networks\",\n    \"title\": \"neural networks\"\n  }, \"[[neural networks]]\"), \" are good at a wide range of tasks, they aren't good at knowing \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"when\"), \" and \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"what\"), \" they don't know.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Part of the problem with this is that there is no ground truth for uncertainty; not something that can be easily testable. \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Same problem associated with quantifying #generalization since the data we need is by definition not available to us.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"This paper looks at using two ways to measure uncertainty from neural networks: \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/calibration\",\n    \"title\": \"calibration\"\n  }, \"[[calibration]]\"), \" and \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/domain-shift\",\n    \"title\": \"domain shift\"\n  }, \"[[domain shift]]\"), \" or \\\"out-of-distribution examples\\\".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Interpretation of \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/dropout\",\n    \"title\": \"dropout\"\n  }, \"[[dropout]]\"), \" uncertainties by Gal and Ghahramani as both \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/MCMC\",\n    \"title\": \"MCMC\"\n  }, \"[[MCMC]]\"), \" sampling, as well as \\\"creation\\\" of [\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"ensembles\",\n    \"title\": \"ensembles\"\n  }, \"ensembles\"), \"] of neural networks.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Fundamentally, the question is whether or not ensembles of neural networks are any good at providing good uncertainty estimates.\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"blockquote\", {\n    parentName: \"li\"\n  }, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/bayesian-model-averaging\",\n    \"title\": \"bayesian model averaging\"\n  }, \"[[bayesian model averaging]]\"), \" assumes that the true model lies within the hypothesis class of the prior, and performs \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"soft model selection\"), \" to find the single best model within the hypothesis class.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"blockquote\", {\n    parentName: \"li\"\n  }, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"...ensembles can be expected to be better when the true model does not lie within the hypothesis class.\"))))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A deep ensemble is supposed to be easier to implement and train than [\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"bayesian-neural-networks\",\n    \"title\": \"bayesian-neural-networks\"\n  }, \"bayesian-neural-networks\"), \"], either variational or MCMC.\")), mdx(\"h3\", {\n    \"id\": \"calibration\"\n  }, \"Calibration\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"This figure shows the performance of the approach w.r.t. a simple 1D regression task, where $y = x^3 + \\\\varepsilon$ \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"561px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/ml-reviews/static/d3557e106e6bf95b5ddd8b8da67259ff/f0685/2021-08-20-10-38-27.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"38.57142857142858%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABf0lEQVQoz01Si26DMAzk/z+uK4XSFzBV5RVCgEIbIEBvdtikIVm2L3fGB3GmacIwDDCzgTEGWo+2Hwmf5xkTYa+X3jhUMzYMpNEDWMsYa7n+fD5wtNYYf8ld26MULQ3oMY4jzDjhcc+R55IENID6vuuRJhJClNSPmEmnZI0ovIMfp+97SKHQqhbv7m0HCyGR3DM0RHzSS7R+I3nkEGmBpmqx0JaPJMU3DXk9e1SlQl03NI425FWb5oVlma0dYybU6mltrp/V2mEXSnV0Nlseh5QVDaHh62KDtXbDsiyJXCFJErJbElHayPMMks8qRXVOYgVRCBRFAdbUdW3rgs7+cP5Mjrvf43gM4Hs+Dq6LIAhs9jwP3uEA3/fxtdvB3bvEO264t+F71lJm3m73hbZ9wmFSHMcIwxDn0wnn8xmXy4XyyYqvVHMfhjfEUYzb7YYoimxciHu9Xi3Gwf/DYatsKeWc5dZ6VVXIstTi3Kdpiq7rtititivyP7Zvb7CuK34AxPVYhKbTrVgAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"2021 08 20 10 38 27\",\n    \"title\": \"2021 08 20 10 38 27\",\n    \"src\": \"/ml-reviews/static/d3557e106e6bf95b5ddd8b8da67259ff/410f3/2021-08-20-10-38-27.png\",\n    \"srcSet\": [\"/ml-reviews/static/d3557e106e6bf95b5ddd8b8da67259ff/0d3e1/2021-08-20-10-38-27.png 140w\", \"/ml-reviews/static/d3557e106e6bf95b5ddd8b8da67259ff/6b1e2/2021-08-20-10-38-27.png 281w\", \"/ml-reviews/static/d3557e106e6bf95b5ddd8b8da67259ff/410f3/2021-08-20-10-38-27.png 561w\", \"/ml-reviews/static/d3557e106e6bf95b5ddd8b8da67259ff/f0685/2021-08-20-10-38-27.png 835w\"],\n    \"sizes\": \"(max-width: 561px) 100vw, 561px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n  \"), \"\\n    \"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"First panel is empirical variance. \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Probably not much because each NN converges to the same weights?\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Second panel is training NLL using a single NN, third smooths the process with adversarial training.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Final panel combines NLL + AT with five NNs.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Other comparison made is on a few tabular datasets; deep ensembles typically have larger RMSE but better NLL.\")), mdx(\"h3\", {\n    \"id\": \"domain-shift\"\n  }, \"Domain shift\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Test predictions on data from \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"unseen\"), \" classes: uncertainties should be proportional to distance from training data.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Model is trained on a standard \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/MNIST\",\n    \"title\": \"MNIST\"\n  }, \"[[MNIST]]\"), \" train/test split, however an additional testing class of \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/NotMNIST\",\n    \"title\": \"NotMNIST\"\n  }, \"[[NotMNIST]]\"), \".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"blockquote\", {\n    parentName: \"li\"\n  }, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \" We do not have access to the true conditional probabilities, but we expect the predictions to be closer to uniform on unseen classes compared to the known classes where the predictive probabilities should concentrate on the true targets\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The left panel shows how the distribution in \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/entropy\",\n    \"title\": \"entropy\"\n  }, \"[[entropy]]\"), \" is much broader/uniform for the ensemble + AT case, compared to the dropout uncertainties which are also much more discrete. \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"355px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"80%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAACNElEQVQ4y42UW3ObMBCF/f//TB86fel7n9rpJG0mNW58wWDuIEAgcRE6XckmxYlnYs0wsOjo26PLagVqeZZit93CcRys1w62ux0YK3E4uJBSYm5SCmxI4/u+1TrOBru9a+OqrKxmpZTCMAyQbQshBDpeQVYMg2gtbBxHaK3tY76HTqLvOoiGoykyqxdNY7UTsSxQTxOmIrYZpppBFSkUQeu6xmT65of6dZVDyxYYehqTUlygrUr0ZMpoVsoIxwFj4p9dtA0GA6KMSZKgJdc20QWo8hhTy6HIbV9XUDQzXjJkeW7Hn4GUTaUBNA3oeY2uLC24oakIIf8DtQFGlKyF6nvSMQvtKCnnfOGw7zAS8Fabp6vfAG81ZdfwAlQX4LwB82PXmN4GqkychZg6+U5r+z8CzlmvgbQ0/TVw/r4CjncDyeE9wI8dKrtp/PkBXRrf51BfOrB4vwUW37+hz9OzxmzWLaCiKZjde91ZOkZ6sXPLg80ef6ANTxTrV/28JKaSVva3GjD4OygqJ+Hu0bxsIE6+UV4NMo2vfyP8+gXe509oSGtmt2yrqqqQUUVET49I188okxgNHdI68JEdjyiyDEJ26Okg51QN8fYF0cNPROs/iJ9+IfvrgMUxqqJAUbAz0Nw2e/cI1/PgnQJ7e7gUB0EATmXYUrV0dCEwVuDo+TiQzvVPiLOcYg97up04VZgFzlZzcpKSuyiibFR6YRCiZOxdNZgEWZogDEMwcmXqPSaH85r+A51Q2Fnq3NaHAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"2021 08 20 11 14 39\",\n    \"title\": \"2021 08 20 11 14 39\",\n    \"src\": \"/ml-reviews/static/ecad5890bae970f3f7bbfc2ef55b4fcd/526ee/2021-08-20-11-14-39.png\",\n    \"srcSet\": [\"/ml-reviews/static/ecad5890bae970f3f7bbfc2ef55b4fcd/0d3e1/2021-08-20-11-14-39.png 140w\", \"/ml-reviews/static/ecad5890bae970f3f7bbfc2ef55b4fcd/6b1e2/2021-08-20-11-14-39.png 281w\", \"/ml-reviews/static/ecad5890bae970f3f7bbfc2ef55b4fcd/526ee/2021-08-20-11-14-39.png 355w\"],\n    \"sizes\": \"(max-width: 355px) 100vw, 355px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Define a \\\"confidence\\\" value $0 \\\\leq \\\\tau \\\\leq 1$ defined as the highest probability of a class. \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"It shouldn't always be one for unseen examples.\"), \" \")), mdx(\"h2\", {\n    \"id\": \"strategy\"\n  }, \"Strategy\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"blockquote\", {\n    parentName: \"li\"\n  }, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Given the input features $x$, we use a neural network to model the probabilistic predictive distribution $p_\\\\theta(y \\\\vert x)$ over the labels, where $\\\\theta$ are the parameters of the NN.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Three steps:\", mdx(\"ol\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Use a proper \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/scoring-rule\",\n    \"title\": \"scoring rule\"\n  }, \"[[scoring rule]]\"), \"; $-S(p_\\\\theta, y \\\\vert x) \\\\in \\\\mathcal{L}$ where $S$ can be \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/maximum-likelihood\",\n    \"title\": \"maximum likelihood\"\n  }, \"[[maximum likelihood]]\"), \"/\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/mean-squared-error\",\n    \"title\": \"mean squared error\"\n  }, \"[[mean squared error]]\"), \"/\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/softmax\",\n    \"title\": \"softmax\"\n  }, \"[[softmax]]\"), \". MSE is also known as the \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/Brier-score\",\n    \"title\": \"Brier score\"\n  }, \"[[Brier score]]\"), \".\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/Adversarial-training\",\n    \"title\": \"Adversarial training\"\n  }, \"[[Adversarial training]]\"), \" to smooth the predictive distribution\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Train an ensemble \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"lol\"))))), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"561px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/ml-reviews/static/0e9de60ce54afb3da942ba2848707d3d/ace37/2021-08-20-10-33-20.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"30%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABGElEQVQY001Q2W6EMAzk//9t0VZbCIHlhhDCVegD2iIxtU1X6oPlI5PxjD3f93G7+QjDECpU0FpDRxEiCu7jWCMIAiRJgvT5FFzTNJimCeM4YhgGCecclmWB5/oeSikkcXyR/RHGlBV9fpNH6spMGBE+z3JUZYk8z2HaFlVVoaTe+3m90NKgpgFn21kCVsjSFF3Xoe8dhZXaWiuqRlJkuZf3XtTO8yzh8VDRdud6ZLSVAdu6ytaV8nEc2Pcd53nie9vE2kDBxMYYEcEzrp0b4H2R74/7HY/HA58UbLmuSrKnEdLtirwQdRuRsT1+Z6vxvxPxnB0KIR+SB0VRiiq21ZlODs3HN+ay9bbMeLZY17Xg5Vxcs0LC/QIRbbbSQ3ds4AAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"2021 08 20 10 33 20\",\n    \"title\": \"2021 08 20 10 33 20\",\n    \"src\": \"/ml-reviews/static/0e9de60ce54afb3da942ba2848707d3d/410f3/2021-08-20-10-33-20.png\",\n    \"srcSet\": [\"/ml-reviews/static/0e9de60ce54afb3da942ba2848707d3d/0d3e1/2021-08-20-10-33-20.png 140w\", \"/ml-reviews/static/0e9de60ce54afb3da942ba2848707d3d/6b1e2/2021-08-20-10-33-20.png 281w\", \"/ml-reviews/static/0e9de60ce54afb3da942ba2848707d3d/410f3/2021-08-20-10-33-20.png 561w\", \"/ml-reviews/static/0e9de60ce54afb3da942ba2848707d3d/ace37/2021-08-20-10-33-20.png 666w\"],\n    \"sizes\": \"(max-width: 561px) 100vw, 561px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n  \"), \"\\n    \")), mdx(\"h3\", {\n    \"id\": \"regression\"\n  }, \"Regression\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Typical regression NN will minimize MSE, i.e. the output corresponds to the regression mean. Here, the authors suggest to output \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"two values\"), \": the predicted mean and variance. The variance is done via $\\\\sigma = (1 + \\\\exp(x)) + \\\\varepsilon$ where $\\\\varepsilon$ is a small positive number of numerical stability.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/negative-log-likelihood\",\n    \"title\": \"negative log-likelihood\"\n  }, \"[[negative log-likelihood]]\"), \" criterion used:\\n$$ -\\\\log p\", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"\\\\theta(y_n \\\\vert x_n) = \\\\frac{\\\\log \\\\sigma^2\"), \"\\\\theta(x)}{2} + \\\\frac{(y - \\\\mu\", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"\\\\theta(x))^2}{2\\\\sigma^2\"), \"\\\\theta(x)} + C$$\")), mdx(\"p\", null, \"Julia implementation:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-julia\"\n  }, \"begin\\n  function nll(mu, var, y)\\n    return log(var^2) / 2 + (y - mu)^2 / (2 * var^2) + eps(Float32)\\n  end\\nend\\n\")), mdx(\"h3\", {\n    \"id\": \"adversarial-training\"\n  }, \"Adversarial training\"), mdx(\"p\", null, \"Basically use conventional \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/fast-gradient-sign-method\",\n    \"title\": \"fast gradient sign method\"\n  }, \"[[fast gradient sign method]]\"), \", or \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/virtual-adversarial-training\",\n    \"title\": \"virtual adversarial training\"\n  }, \"[[virtual adversarial training]]\"), \".\"), mdx(\"h3\", {\n    \"id\": \"ensemble-training\"\n  }, \"Ensemble training\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Choice between \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/decision-trees\",\n    \"title\": \"decision trees\"\n  }, \"[[decision trees]]\"), \" or [\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"boosting\",\n    \"title\": \"boosting\"\n  }, \"boosting\"), \"]; the authors chose the former because they are much better for distributed training.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"There is an extensive discussion into decorrelation of trees with \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/bootstrapping\",\n    \"title\": \"bootstrapping\"\n  }, \"[[bootstrapping]]\"), \"; the authors instead just use random initialization of the parameters, and shuffle the training data points for each \\\"tree\\\" in practice.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Apparently [\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"stochastic-multiple-choice-learning\",\n    \"title\": \"stochastic-multiple-choice-learning\"\n  }, \"stochastic-multiple-choice-learning\"), \"] helps with de-correlating the NN \\\"trees\\\".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Ensemble as a uniformly-weighted \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/mixture-model\",\n    \"title\": \"mixture model\"\n  }, \"[[mixture model]]\"), \"\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"For classification, averaging the predicted probabilities\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"For regression, Gaussian mixture, with a mixture mean $\\\\mu\", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"*(x)$ and variance $\\\\sigma\"), \"*(x)$\")))), mdx(\"p\", null, \"$$\\\\mu\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"*(x) = \\\\frac{1}{M}\\\\sum_m \\\\mu\"), \"{\\\\theta_m}(x)$$\"), mdx(\"p\", null, \"$$\\\\sigma\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"*^2(x) = \\\\frac{1}{M}\\\\sum_m(\\\\sigma^2\"), \"{\\\\theta\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"m}(x) + \\\\mu^2\"), \"{\\\\theta\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"m}(x)) - \\\\mu\"), \"*^2(x)$$\"), mdx(\"p\", null, \"A naive Julia implementation:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-julia\"\n  }, \"begin\\n  \\\"\\\"\\\"Reduce the predictions from each tree into an ensemble\\n  mean and variance. Assumes `means` and `vars` are a list\\n  of vectors.\\n  \\\"\\\"\\\"\\n  function reduce_ensemble(means, vars)\\n    means = reduce(hcat, means)\\n    ensemble_mean = mean(means, dims=2)\\n    vars = reduce(hcat, vars)\\n    ensemble_var = mean(means.^2 .+ vars.^2, dims=2) .- ensemble_mean.^2\\n    return ensemble_mean, ensemble_var\\n  end\\n\\n  \\\"\\\"\\\"Given a struct `ensemble`, and inputs `x`, we loop over\\n  each tree in the ensemble/forest (also assuming tree is a functor)\\n  and collect up all of the means/variances predicted by each tree.\\n  We then bag the results.\\n  \\\"\\\"\\\"\\n  function predict(x, ensemble)\\n    means, vars = [], []\\n    for tree in ensemble.forest\\n      mean, var = tree(x)\\n      push!(means, mean)\\n      push!(vars, var)\\n    end\\n    ensemble_mean, ensemble_var = reduce_ensemble(means, vars)\\n  end\\nend\\n\")), mdx(\"h2\", {\n    \"id\": \"comments\"\n  }, \"Comments\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Good for #distributed training; maybe a good use case for #intel-research.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"5 NN's seem good enough for uncertainty estimation, but not sure how this really actually scales for real tasks.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"How does it really fit into a hypothesis testing context?\")), mdx(\"p\", null, \"[scoring rule]\", \": scoring rule \\\"scoring rule\\\"\\n\", \"[maximum likelihood]\", \": maximum likelihood \\\"maximum likelihood\\\"\\n\", \"[Adversarial training]\", \": Adversarial training \\\"Adversarial training\\\"\\n\", \"[negative log-likelihood]\", \": negative log-likelihood \\\"negative log-likelihood\\\"\\n\", \"[fast gradient sign method]\", \": fast gradient sign method \\\"fast gradient sign method\\\"\\n\", \"[virtual adversarial training]\", \": virtual adversarial training \\\"virtual adversarial training\\\"\"));\n}\n;\nMDXContent.isMDXComponent = true;","parent":{"__typename":"File","id":"3f62869b-c6df-5c74-bc98-e7fcbbfb615d","fields":{"slug":"/notes/scalable-uncertainties-from-deep-ensembles","title":"scalable-uncertainties-from-deep-ensembles"}}}]},"fields":{"slug":"/notes/scoring rule","title":"scoring rule"}}},"pageContext":{"id":"407fb1d4-b936-5a07-b418-60e84569c552"}},"staticQueryHashes":["2098632890","2221750479","2468095761"]}