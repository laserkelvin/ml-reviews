{
    "componentChunkName": "component---node-modules-gatsby-theme-kb-src-templates-topic-js",
    "path": "/notes/variational autoencoder",
    "result": {"data":{"file":{"childMdx":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": \"needs-expanding\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"variational autoencoder\"), mdx(\"p\", null, \"A probabilistic \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/architecture\",\n    \"title\": \"architecture\"\n  }, \"[[architecture]]\"), \" from the family of \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"autoencoders\",\n    \"title\": \"autoencoders\"\n  }, \"[[autoencoders]]\"), \". The self-learned representation is used to parameterize a probability distribution (i.e. the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/posterior-distribution\",\n    \"title\": \"posterior distribution\"\n  }, \"[[posterior distribution]]\"), \"), from which a decoder can draw samples from to generate a range of outputs. We can either directly predict the mean of the distribution, or perform sampling over the distribution to obtain #uncertainty estimates.\"), mdx(\"p\", null, \"Is a sub-category of \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"bayesian-neural-networks\",\n    \"title\": \"bayesian-neural-networks\"\n  }, \"[[bayesian-neural-networks]]\"), \", whereby variational inference is done, usually in contrast to full \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/MCMC\",\n    \"title\": \"MCMC\"\n  }, \"[[MCMC]]\"), \" sampling.\"), mdx(\"p\", null, \"Problem with VAEs is that they are susceptible to \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/posterior-collapse\",\n    \"title\": \"posterior collapse\"\n  }, \"[[posterior collapse]]\"), \", where a single output is produced regardless: this is when the decoder ignores the latent variable completely, and becomes a deterministic model.\"), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html\"\n  }, \"This blog by Lilian Weng\"), \" has an incredibly in-depth description of the theory behind VAEs.\"), mdx(\"h2\", null, \"Types of VAEs\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/beta-VAE\",\n    \"title\": \"beta-VAE\"\n  }, \"[[beta-VAE]]\"), \" provides a tuning parameter, \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"\\u03B2\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\beta\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.8889em\",\n      \"verticalAlign\": \"-0.1944em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.05278em\"\n    }\n  }, \"\\u03B2\"))))), \", that attempts to force disentangling of the latent vector into unit Gaussian priors. Relatively easy to understand and implement, however quite readily suffers from \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/posterior-collapse\",\n    \"title\": \"posterior collapse\"\n  }, \"[[posterior collapse]]\"), \".\")), mdx(\"p\", null, \"[posterior collapse]\", \": posterior collapse \\\"posterior collapse\\\"\\n\", \"[posterior collapse]\", \": posterior collapse \\\"posterior collapse\\\"\"), mdx(\"p\", null, \"[posterior collapse]\", \": posterior collapse \\\"posterior collapse\\\"\\n\", \"[posterior collapse]\", \": posterior collapse \\\"posterior collapse\\\"\"), mdx(\"p\", null, \"[posterior collapse]\", \": posterior collapse \\\"posterior collapse\\\"\\n\", \"[posterior collapse]\", \": posterior collapse \\\"posterior collapse\\\"\"));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"title":"","private":false},"outboundReferences":[{"contextLine":"A probabilistic [[architecture]] from the family of [[autoencoders]]. The self-learned representation is used to parameterize a probability distribution (i.e. the [[posterior distribution]]), from which a decoder can draw samples from to generate a range of outputs. We can either directly predict the mean of the distribution, or perform sampling over the distribution to obtain #uncertainty estimates.","targetAnchor":null,"refWord":"uncertainty","target":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"uncertainty\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"#aleatoric uncertainty is statistical uncertainty, i.e. in the frequentist view noisy measurements.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"#epistemic uncertainty is systematic uncertainty, i.e. model aspects not considered, such as physical constraints like gravity, etc.\")), mdx(\"h2\", null, \"Reinforcement learning\"), mdx(\"p\", null, \"In the context of \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"reinforcement%20learning\",\n    \"title\": \"reinforcement learning\"\n  }, \"[[reinforcement learning]]\"), \", according to \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/decision-making-book\",\n    \"title\": \"decision-making-book\"\n  }, \"[[decision-making-book]]\"), \" uncertainty comprises:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"outcome uncertainty\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"what effect will our action have\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"model uncertainty\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"our model of the problem is uncertain\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"state uncertainty\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"the true state is uncertain\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"can be modelled as a \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/partially-observable-markov-decision-process\",\n    \"title\": \"partially-observable-markov-decision-process\"\n  }, \"[[partially-observable-markov-decision-process]]\"), \"\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"interaction uncertainty\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"behavior of other agents is uncertain\")))));\n}\n;\nMDXContent.isMDXComponent = true;","parent":{"id":"bd86e722-d930-5c0d-8e57-d72278691df5","fields":{"slug":"/notes/uncertainty","title":"uncertainty"}}}},{"contextLine":"A probabilistic [[architecture]] from the family of [[autoencoders]]. The self-learned representation is used to parameterize a probability distribution (i.e. the [[posterior distribution]]), from which a decoder can draw samples from to generate a range of outputs. We can either directly predict the mean of the distribution, or perform sampling over the distribution to obtain #uncertainty estimates.","targetAnchor":null,"refWord":"autoencoders","target":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"autoencoders\"), mdx(\"p\", null, \"A type of machine learning \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/architecture\",\n    \"title\": \"architecture\"\n  }, \"[[architecture]]\"), \" that encodes data into a self-learned representation.\"));\n}\n;\nMDXContent.isMDXComponent = true;","parent":{"id":"a4f94698-6ed6-5e36-bae3-d4b2fc873702","fields":{"slug":"/notes/autoencoders","title":"autoencoders"}}}},{"contextLine":"Is a sub-category of [[bayesian-neural-networks]], whereby variational inference is done, usually in contrast to full [[MCMC]] sampling.","targetAnchor":null,"refWord":"bayesian-neural-networks","target":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"bayesian-neural-networks\"), mdx(\"p\", null, \"\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/neural-networks\",\n    \"title\": \"neural networks\"\n  }, \"[[neural networks]]\"), \" in a \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/bayesian\",\n    \"title\": \"bayesian\"\n  }, \"[[bayesian]]\"), \" formalism. The general idea is that the initialization schemes for neural networks act as priors, and by training the network, we end up with a parameterized \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/posterior-distribution\",\n    \"title\": \"posterior distribution\"\n  }, \"[[posterior distribution]]\"), \".\"));\n}\n;\nMDXContent.isMDXComponent = true;","parent":{"id":"f9da958d-5c93-5082-898e-0f3d0a4b0dd1","fields":{"slug":"/notes/bayesian-neural-networks","title":"bayesian-neural-networks"}}}},{"contextLine":"Problem with VAEs is that they are susceptible to [[posterior collapse]], where a single output is produced regardless: this is when the decoder ignores the latent variable completely, and becomes a deterministic model.","targetAnchor":null,"refWord":"posterior collapse","target":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"posterior collapse\"), mdx(\"p\", null, \"An issue primarily related to \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/variational-autoencoder\",\n    \"title\": \"variational autoencoder\"\n  }, \"[[variational autoencoder]]\"), \" models, where the output becomes deterministic in a supposedly probabilistic model.\"), mdx(\"p\", null, \"Some annealing schemes, such as \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"cyclical-annealing-schedule-a-simple-approach-to-mitigating-kl-vanishing\",\n    \"title\": \"cyclical-annealing-schedule-a-simple-approach-to-mitigating-kl-vanishing\"\n  }, \"[[cyclical-annealing-schedule-a-simple-approach-to-mitigating-kl-vanishing]]\"), \" is developed to help VAEs learn to encode good latent variables \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"and\"), \" good accuracy in the outputs.\"), mdx(\"p\", null, \"[variational autoencoder]\", \": variational autoencoder \\\"variational autoencoder\\\"\"));\n}\n;\nMDXContent.isMDXComponent = true;","parent":{"id":"568fb177-3b40-5259-99b5-3a6c253faa0f","fields":{"slug":"/notes/posterior collapse","title":"posterior collapse"}}}},{"contextLine":"- [[beta-VAE]] provides a tuning parameter, $\\beta$, that attempts to force disentangling of the latent vector into unit Gaussian priors. Relatively easy to understand and implement, however quite readily suffers from [[posterior collapse]].","targetAnchor":null,"refWord":"posterior collapse","target":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"posterior collapse\"), mdx(\"p\", null, \"An issue primarily related to \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/variational-autoencoder\",\n    \"title\": \"variational autoencoder\"\n  }, \"[[variational autoencoder]]\"), \" models, where the output becomes deterministic in a supposedly probabilistic model.\"), mdx(\"p\", null, \"Some annealing schemes, such as \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"cyclical-annealing-schedule-a-simple-approach-to-mitigating-kl-vanishing\",\n    \"title\": \"cyclical-annealing-schedule-a-simple-approach-to-mitigating-kl-vanishing\"\n  }, \"[[cyclical-annealing-schedule-a-simple-approach-to-mitigating-kl-vanishing]]\"), \" is developed to help VAEs learn to encode good latent variables \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"and\"), \" good accuracy in the outputs.\"), mdx(\"p\", null, \"[variational autoencoder]\", \": variational autoencoder \\\"variational autoencoder\\\"\"));\n}\n;\nMDXContent.isMDXComponent = true;","parent":{"id":"568fb177-3b40-5259-99b5-3a6c253faa0f","fields":{"slug":"/notes/posterior collapse","title":"posterior collapse"}}}}],"inboundReferences":[{"contextLine":"- [[variational autoencoder]]","referrer":{"parent":{"id":"be8ae56f-15a1-5eec-9f16-43ec9bc7a323","fields":{"slug":"/readme","title":"ML Reviews"}}}},{"contextLine":"So that we force the embeddings to learn unit Gaussians similar to the $\\beta$-regularization in [[variational autoencoder]].","referrer":{"parent":{"id":"9c507678-b215-5271-af72-a5590078fdcc","fields":{"slug":"/notes/vicreg","title":"vicreg: Variance-invariance-covariance regularization for self-supervised learning"}}}},{"contextLine":"- [[variational autoencoder]]","referrer":{"parent":{"id":"e8a54739-a1b8-5710-9406-99e2c6e3a726","fields":{"slug":"/notes/machine-learning-notes","title":"machine-learning-notes"}}}},{"contextLine":"An issue primarily related to [[variational autoencoder]] models, where the output becomes deterministic in a supposedly probabilistic model.","referrer":{"parent":{"id":"568fb177-3b40-5259-99b5-3a6c253faa0f","fields":{"slug":"/notes/posterior collapse","title":"posterior collapse"}}}}]},"fields":{"slug":"/notes/variational autoencoder","title":"variational autoencoder"}}},"pageContext":{"id":"0dff8916-86db-5c4c-bdee-c0c7be1cb322","refWordMdxSlugDict":{"uncertainty":"notes/uncertainty","reinforcement learning":"notes/reinforcement-learning","agent":"notes/agent","autoencoders":"notes/autoencoders","bayesian-neural-networks":"notes/bayesian-neural-networks","posterior collapse":"notes/posterior-collapse","variational autoencoder":"notes/variational-autoencoder","cyclical-annealing-schedule-a-simple-approach-to-mitigating-kl-vanishing":"notes/cyclical-annealing-schedule-a-simple-approach-to-mitigating-kl-vanishing"}}},
    "staticQueryHashes": ["2221750479","2380733210","2768355698","63159454","847517413"]}