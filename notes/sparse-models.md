# sparse-models

Efficient representations of models that only contain "useful" information; sparisfication is the act of removing redundant weights from models, which result in smaller and faster models.

The interest for this in #intel-research would be scalable, improved CPU performance for tasks.

This [paper](https://neuralmagic.com/wp-content/uploads/2021/02/Sparsity-in-Deep-Learning-Pruning-and-growth-for-efficient-inference-and-training-in-neural-networks-.pdf) provides a very detailed review which I need to go throughput.