# bayes-rule

Canonically, for events $x$ and $y$, we have the definition:

$$p(x \vert y) = \frac{p(y \vert x)p(x)}{p(y)}$$

where $p(y)$ is the [[evidence]], $p(x)$ is the [[prior]], $p(x \vert y)$ is the [[posterior distribution]], and $p(y \vert x)$ is the [[conditional-distribution]] known as the likelihood.

Another way written, as seen in [Josh Speagle's slides](https://speakerdeck.com/joshspeagle/an-introduction-to-dynamic-nested-sampling?slide=4), is to write it as:

$$\text{Pr}(\Theta \vert D,M) = \frac{\text{Pr}(D \vert \Theta, M)\text{Pr}(\Theta \vert M)}{\text{Pr}(D \vert M)}$$

where aspects are more clearly defined as $M$ model dependent (such as the prior).


[//begin]: # "Autogenerated link references for markdown compatibility"
[evidence]: evidence "evidence"
[conditional-distribution]: conditional-distribution "conditional-distribution"
[//end]: # "Autogenerated link references"